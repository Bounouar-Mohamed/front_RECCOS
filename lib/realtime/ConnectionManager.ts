/**
 * ConnectionManager - Gestion de la connexion Realtime WebRTC
 * 
 * Utilise le SDK @openai/agents-realtime pour √©tablir une connexion
 * WebRTC directe entre le navigateur et l'API OpenAI Realtime.
 * 
 * Le SDK g√®re automatiquement via WebRTC :
 * - L'acc√®s au microphone
 * - La lecture audio via un √©l√©ment <audio> auto-cr√©√©
 * - La connexion avec OpenAI
 */

import { 
  RealtimeAgent, 
  RealtimeSession,
  RealtimeSessionConnectOptions,
} from '@openai/agents-realtime';
import { tool } from '@openai/agents-core';
import { apiClient } from '../api/client';
import { 
  RealtimeConfig, 
  ConnectionStatus, 
  AudioState,
  RealtimeSessionEvents,
  RealtimeMessage,
  RealtimeSamplingConfig,
} from './types';
import { 
  REALTIME_TOKEN_ENDPOINT,
  REALTIME_CONFIG_ENDPOINT,
  RECONNECT_DELAY, 
  MAX_RECONNECT_ATTEMPTS 
} from './config';
import { buildVoiceInstructions } from './voiceInstructions';

interface ConnectionManagerOptions {
  config: RealtimeConfig;
  events: Partial<RealtimeSessionEvents>;
}

interface TokenResponse {
  data: {
    token: string;
    expires_in?: number;
    sessionId?: string;
    assistant_thread_id?: string;
  };
}

interface TokenRequest {
  userId?: string;
  tenantId?: string;
  conversationId?: string;
}

interface BackendConfig {
  model: string;
  voice: string;
  systemInstructions: string;
  sampling?: RealtimeSamplingConfig;
  features: {
    bargeInEnabled: boolean;
    vadThreshold: number;
    silenceDurationMs: number;
    inputTranscription?: boolean;
    supportedLocales: string[];
  };
  tools?: any[];
}

export class ConnectionManager {
  private agent: RealtimeAgent | null = null;
  private session: RealtimeSession | null = null;
  private config: RealtimeConfig;
  private backendConfig: BackendConfig | null = null;
  private events: Partial<RealtimeSessionEvents>;
  private status: ConnectionStatus = 'disconnected';
  private audioState: AudioState = 'idle';
  private reconnectAttempts = 0;
  private sessionId: string | null = null;
  private threadId: string | null = null;
  private currentTranscript: string = '';
  private audioElement: HTMLAudioElement | null = null;

  constructor(options: ConnectionManagerOptions) {
    this.config = options.config;
    this.events = options.events;
  }

  /**
   * Cr√©e des tools proxy qui appellent le backend
   * Ces tools sont ex√©cut√©s c√¥t√© serveur via l'endpoint /chatbot/realtime/tools/execute
   */
  private createBackendTools(toolDefinitions: any[]): any[] {
    if (!toolDefinitions || toolDefinitions.length === 0) {
      console.log('[ConnectionManager] No tools to create');
      return [];
    }

    console.log('[ConnectionManager] Creating backend tools:', toolDefinitions.map(t => t.name));

    return toolDefinitions.map(toolDef => {
      return tool({
        name: toolDef.name,
        description: toolDef.description || '',
        parameters: toolDef.parameters || { type: 'object', properties: {} },
        strict: false,
        needsApproval: false, // Auto-execute, pas besoin d'approbation
        execute: async (input: unknown) => {
          console.log(`[ConnectionManager] üîß Executing tool: ${toolDef.name}`, input);
          
          try {
            const correlationId = `corr_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`;
            const response = await apiClient.post('/chatbot/realtime/tools/execute', {
              name: toolDef.name,
              arguments: input,
              sessionId: this.sessionId || `sess_${Date.now()}`,
              userId: 'anonymous', // TODO: get from auth context
              correlationId,
            });
            
            const result = (response.data as any)?.output || (response.data as any)?.data?.output || response.data;
            console.log(`[ConnectionManager] ‚úÖ Tool ${toolDef.name} result:`, typeof result === 'object' ? JSON.stringify(result).substring(0, 200) : result);
            
            // Retourner le r√©sultat en string pour le mod√®le
            return typeof result === 'string' ? result : JSON.stringify(result);
          } catch (error: any) {
            console.error(`[ConnectionManager] ‚ùå Tool ${toolDef.name} error:`, error.response?.data || error.message);
            return JSON.stringify({ 
              error: true, 
              message: error.response?.data?.message || error.message || 'Tool execution failed' 
            });
          }
        },
      });
    });
  }

  /**
   * R√©cup√®re la configuration realtime depuis le backend
   * Cette config contient le mod√®le et la voix utilis√©s par Quantix
   */
  private async getBackendConfig(): Promise<BackendConfig> {
    try {
      console.log('[ConnectionManager] Fetching backend config...');
      const response = await apiClient.get<{ data: BackendConfig }>(
        REALTIME_CONFIG_ENDPOINT,
        { skipAuthRedirect: true } as any
      );
      
      // La r√©ponse peut √™tre directement l'objet ou wrapp√©e dans data
      const config = (response.data as any).model 
        ? (response.data as unknown as BackendConfig)
        : response.data.data;
      
      console.log('[ConnectionManager] Backend config received:', {
        model: config.model,
        voice: config.voice,
      });
      
      return config;
    } catch (error) {
      console.warn('[ConnectionManager] Failed to fetch backend config, using defaults:', error);
      // Fallback sur la config locale
      return {
        model: this.config.model,
        voice: this.config.voice,
        systemInstructions: this.config.systemInstructions,
        sampling: this.config.sampling,
        features: {
          bargeInEnabled: this.config.features.bargeInEnabled,
          vadThreshold: this.config.features.vadThreshold,
          silenceDurationMs: this.config.features.silenceDurationMs,
          inputTranscription: this.config.features.inputTranscription,
          supportedLocales: ['fr', 'en'],
        },
      };
    }
  }

  /**
   * R√©cup√®re un token ephemeral depuis le backend
   */
  private async getEphemeralToken(): Promise<string> {
    try {
      let userId: string | undefined;
      let tenantId: string | undefined;
      
      if (typeof window !== 'undefined') {
        const userStr = localStorage.getItem('user');
        if (userStr) {
          try {
            const user = JSON.parse(userStr);
            userId = user.id || user.userId;
            tenantId = user.tenantId;
          } catch {
            // Ignore parse error
          }
        }
      }

      const requestBody: TokenRequest = { userId, tenantId };

      console.log('[ConnectionManager] Requesting ephemeral token...');
      const response = await apiClient.post<TokenResponse>(
        REALTIME_TOKEN_ENDPOINT,
        requestBody,
        { skipAuthRedirect: true } as any
      );

      if (response.data.data.sessionId) {
        this.sessionId = response.data.data.sessionId;
      }
      if (response.data.data.assistant_thread_id) {
        this.threadId = response.data.data.assistant_thread_id;
      }

      const token = response.data.data.token;
      // Log du token complet pour diagnostic (√† supprimer en production)
      console.log('[ConnectionManager] Got ephemeral token (length):', token?.length);
      console.log('[ConnectionManager] Token prefix:', token?.substring(0, 3));
      if (!token || typeof token !== 'string' || !token.startsWith('ek_')) {
        console.error('[ConnectionManager] ‚ùå Invalid token format:', token);
        throw new Error('Token invalide re√ßu du backend');
      }
      console.log('[ConnectionManager] Got ephemeral token:', token.substring(0, 15) + '...');
      return token;
    } catch (error) {
      console.error('[ConnectionManager] Failed to get ephemeral token:', error);
      throw new Error('Impossible d\'obtenir le token de connexion');
    }
  }

  private setStatus(status: ConnectionStatus): void {
    console.log('[ConnectionManager] Status:', this.status, '->', status);
    this.status = status;
    this.events.onStatusChange?.(status);
  }

  private setAudioState(state: AudioState): void {
    console.log('[ConnectionManager] AudioState:', this.audioState, '->', state);
    this.audioState = state;
    this.events.onAudioStateChange?.(state);
  }

  /**
   * Normalise le payload envoy√© via session.update pour √©viter les erreurs OpenAI
   * CRITIQUE: Cette fonction doit s'assurer que l'audio est correctement configur√©
   */
  private sanitizeSessionUpdatePayload(session: any): Record<string, unknown> {
    const sanitized: Record<string, unknown> = {};

    const defaultVoice =
      typeof (this.agent as any)?.voice === 'string'
        ? (this.agent as any).voice
        : this.backendConfig?.voice || this.config.voice || 'alloy';

    const instructions =
      typeof session?.instructions === 'string'
        ? session.instructions
        : typeof (this.agent as any)?.instructions === 'string'
          ? (this.agent as any).instructions
          : undefined;
    if (instructions) {
      sanitized.instructions = instructions;
    }

    const voice =
      typeof session?.voice === 'string'
        ? session.voice
        : typeof session?.audio?.output?.voice === 'string'
          ? session.audio.output.voice
          : defaultVoice;
    sanitized.voice = voice;

    // CRITIQUE: Toujours forcer audio + text pour que le mod√®le r√©ponde en voix
    const preferredModalities =
      (Array.isArray(session?.modalities) && session.modalities.length > 0
        ? session.modalities
        : Array.isArray(session?.output_modalities) && session.output_modalities.length > 0
          ? session.output_modalities
          : null) || [];
    const normalizedModalities = Array.from(new Set(preferredModalities));
    if (!normalizedModalities.includes('audio') || !normalizedModalities.includes('text')) {
      sanitized.modalities = ['audio', 'text'];
    } else {
      sanitized.modalities = normalizedModalities;
    }

    if (typeof session?.temperature === 'number') {
      sanitized.temperature = session.temperature;
    }

    if (session?.tool_choice) {
      sanitized.tool_choice = session.tool_choice;
    }

    // CRITIQUE: Configurer turn_detection pour que le mod√®le g√©n√®re des r√©ponses automatiquement
    // C'est ESSENTIEL pour que le mod√®le r√©ponde apr√®s que l'utilisateur ait parl√©
    sanitized.turn_detection = {
      type: 'server_vad',
      threshold: 0.5,
      prefix_padding_ms: 300,
      silence_duration_ms: 500,
      create_response: true,  // ‚úÖ CRUCIAL: Le mod√®le doit cr√©er une r√©ponse automatiquement
      interrupt_response: true,
    };

    // NOTE: input_audio_transcription peut causer des erreurs si mal configur√©
    // On l'omet pour le moment pour diagnostiquer le probl√®me de r√©ponse audio

    return sanitized;
  }

  private logAudioElementState(context: string): void {
    if (!this.audioElement) {
      console.warn(`[ConnectionManager] üîà (${context}) Aucun audioElement attach√©`);
      return;
    }

    const { currentTime, muted, volume, readyState, srcObject, networkState } = this.audioElement;
    console.log(`[ConnectionManager] üîà (${context}) audio element state`, {
      currentTime,
      muted,
      volume,
      readyState,
      networkState,
      hasSrcObject: !!srcObject,
    });
  }

  private createAudioElement(): HTMLAudioElement {
    if (typeof document === 'undefined') {
      throw new Error('Document non disponible pour cr√©er un √©l√©ment audio');
    }

    if (this.audioElement) {
      this.audioElement.remove();
      this.audioElement = null;
    }

    const audioElement = document.createElement('audio');
    audioElement.autoplay = true;
    audioElement.id = 'realtime-audio-output';
    audioElement.style.display = 'none';
    
    // CRITIQUE: S'assurer que l'audio n'est pas muted et que le volume est √† 1
    audioElement.muted = false;
    audioElement.volume = 1.0;

    const logEvent = (eventName: string) => () => {
      console.log(`[ConnectionManager] üîà Audio element event: ${eventName}`);
      this.logAudioElementState(`audio:${eventName}`);
    };

    audioElement.addEventListener('play', logEvent('play'));
    audioElement.addEventListener('pause', logEvent('pause'));
    audioElement.addEventListener('volumechange', logEvent('volumechange'));
    audioElement.addEventListener('ended', logEvent('ended'));
    audioElement.addEventListener('loadeddata', logEvent('loadeddata'));
    audioElement.addEventListener('stalled', logEvent('stalled'));
    audioElement.addEventListener('suspend', logEvent('suspend'));
    audioElement.addEventListener('waiting', logEvent('waiting'));
    audioElement.addEventListener('error', () => {
      console.error('[ConnectionManager] üîà Audio element error', audioElement.error);
      this.logAudioElementState('audio:error');
    });
    
    // Log quand il y a r√©ellement de l'audio qui joue
    audioElement.addEventListener('playing', () => {
      console.log('[ConnectionManager] üîä Audio is NOW PLAYING!');
      this.logAudioElementState('audio:playing');
    });
    
    // Log quand les donn√©es arrivent
    audioElement.addEventListener('canplaythrough', () => {
      console.log('[ConnectionManager] üîä Audio can play through!');
    });

    const existingAudio = document.getElementById('realtime-audio-output');
    if (existingAudio) {
      existingAudio.remove();
    }
    document.body.appendChild(audioElement);
    this.audioElement = audioElement;
    this.logAudioElementState('audio:created');
    return audioElement;
  }

  /**
   * Configure les √©v√©nements de la session
   * Ref: realtimeSessionEvents.d.ts
   */
  private setupSessionEvents(): void {
    if (!this.session) return;

    console.log('[ConnectionManager] Setting up session events...');

    // === √âv√©nements audio de l'agent ===
    
    // L'agent commence √† g√©n√©rer de l'audio
    this.session.on('audio_start', () => {
      console.log(`[ConnectionManager] üîä Agent audio_start @ ${new Date().toISOString()}`);
      this.logAudioElementState('agent:audio_start');
      this.setAudioState('speaking');
    });

    // L'agent arr√™te de g√©n√©rer de l'audio
    this.session.on('audio_stopped', () => {
      console.log(`[ConnectionManager] üîá Agent audio_stopped @ ${new Date().toISOString()}`);
      this.logAudioElementState('agent:audio_stopped');
      this.setAudioState('idle');
    });

    // Interruption audio (utilisateur a interrompu l'agent)
    this.session.on('audio_interrupted', () => {
      console.log('[ConnectionManager] ‚ö° Audio interrupted');
      this.logAudioElementState('agent:audio_interrupted');
      this.setAudioState('idle');
    });

    // === √âv√©nements d'historique ===

    // Mise √† jour de l'historique complet
    this.session.on('history_updated', (history: any) => {
      console.log('[ConnectionManager] üìú History updated:', history?.length || 0, 'items');
    });

    // Nouvel √©l√©ment ajout√© √† l'historique
    this.session.on('history_added', (item: any) => {
      console.log('[ConnectionManager] ‚ûï History item added:', item?.type, item?.role);
      
      // Extraire les transcriptions des items
      if (item?.type === 'message') {
        const role = item?.role;
        const content = item?.content?.[0];
        
        if (content?.type === 'input_audio' && content?.transcript) {
          // Transcription de l'utilisateur
          console.log('[ConnectionManager] üë§ User transcript:', content.transcript);
          this.events.onUserTranscript?.(content.transcript, true);
          this.events.onMessage?.({
            id: item.id || crypto.randomUUID(),
            role: 'user',
            content: content.transcript,
            timestamp: new Date(),
            isPartial: false,
          });
        } else if (content?.type === 'audio' && content?.transcript) {
          // Transcription de l'assistant
          console.log('[ConnectionManager] ü§ñ Assistant transcript:', content.transcript);
          this.events.onAssistantTranscript?.(content.transcript, true);
          this.events.onMessage?.({
            id: item.id || crypto.randomUUID(),
            role: 'assistant',
            content: content.transcript,
            timestamp: new Date(),
            isPartial: false,
          });
        } else if (role === 'assistant' && content?.text) {
          // Message texte de l'assistant
          console.log('[ConnectionManager] ü§ñ Assistant text:', content.text);
          this.events.onAssistantTranscript?.(content.text, true);
          this.events.onMessage?.({
            id: item.id || crypto.randomUUID(),
            role: 'assistant',
            content: content.text,
            timestamp: new Date(),
            isPartial: false,
          });
        }
      }
    });

    // === √âv√©nements d'erreur ===

    this.session.on('error', (errorEvent: any) => {
      const payload = errorEvent?.error ?? errorEvent;
      const nested = payload?.error ?? null;
      const message =
        nested?.message ||
        payload?.message ||
        errorEvent?.message ||
        'Session error';
      const code = nested?.code || payload?.code || nested?.error_code;
      const detailedError = message + (code ? ` (code: ${code})` : '');

      console.error('[ConnectionManager] ‚ùå Session error:', {
        event: errorEvent,
        payload,
        nested,
        message: detailedError,
      });

      const error =
        payload instanceof Error
          ? payload
          : new Error(detailedError || 'Session error');

      (error as any).details = nested || payload;
      this.events.onError?.(error);
    });

    // === √âv√©nements du transport (pour debug) ===

    this.session.on('transport_event', (event: any) => {
      // Log TOUS les √©v√©nements de type response.* pour diagnostic
      if (event?.type?.startsWith('response.')) {
        console.log('[ConnectionManager] üì° Response event:', event.type, {
          responseId: event?.response?.id,
          status: event?.response?.status,
          outputItemCount: event?.response?.output?.length,
        });
      }
      
      // Log les √©v√©nements session.* pour voir la config
      if (event?.type?.startsWith('session.')) {
        console.log('[ConnectionManager] üì° Session event:', event.type, {
          modalities: event?.session?.modalities,
          voice: event?.session?.voice,
          turnDetection: event?.session?.turn_detection?.type,
          createResponse: event?.session?.turn_detection?.create_response,
        });
      }
      
      // Log seulement les √©v√©nements importants
      if (event?.type?.includes('speech') || event?.type?.includes('transcription')) {
        console.log('[ConnectionManager] üì° Transport event:', event.type);
      }
      
      // D√©tection de la parole utilisateur via events transport
      if (event?.type === 'input_audio_buffer.speech_started') {
        console.log('[ConnectionManager] üé§ User started speaking');
        this.setAudioState('listening');
      }
      
      if (event?.type === 'input_audio_buffer.speech_stopped') {
        console.log('[ConnectionManager] üé§ User stopped speaking');
        this.setAudioState('thinking');
      }
      
      // CRITIQUE: Log quand le mod√®le commence √† g√©n√©rer une r√©ponse
      if (event?.type === 'response.created') {
        console.log('[ConnectionManager] ü§ñ Response CREATED - model is generating!', {
          responseId: event?.response?.id,
          modalities: event?.response?.modalities,
        });
      }
      
      // Log quand l'audio est g√©n√©r√©
      if (event?.type === 'response.audio.delta') {
        console.log('[ConnectionManager] üîä Audio delta received (model speaking)');
      }
      
      // Log quand la r√©ponse est termin√©e
      if (event?.type === 'response.done') {
        const response = event?.response;
        console.log('[ConnectionManager] ‚úÖ Response DONE', {
          responseId: response?.id,
          status: response?.status,
          usage: response?.usage,
        });
        
        // Si la r√©ponse a √©chou√©, afficher les d√©tails de l'erreur
        if (response?.status === 'failed') {
          console.error('[ConnectionManager] ‚ùå Response FAILED! Details:', {
            status_details: response?.status_details,
            error: response?.error,
            output: response?.output,
            full_response: response,
          });
        }
      }
      
      // Log les erreurs de transcription
      if (event?.type === 'conversation.item.input_audio_transcription.failed') {
        console.error('[ConnectionManager] ‚ùå Transcription FAILED!', {
          error: event?.error,
          item_id: event?.item_id,
          full_event: event,
        });
      }
      
      // Log les erreurs de r√©ponse
      if (event?.type === 'error') {
        console.error('[ConnectionManager] ‚ùå Transport ERROR:', {
          type: event?.error?.type,
          code: event?.error?.code,
          message: event?.error?.message,
          param: event?.error?.param,
          full_error: event?.error,
        });
      }

      // Transcription de l'entr√©e audio
      if (event?.type === 'conversation.item.input_audio_transcription.completed') {
        console.log('[ConnectionManager] üìù User transcription completed:', event.transcript);
        if (event.transcript) {
          this.events.onUserTranscript?.(event.transcript, true);
        }
      }
    });

    // === √âv√©nements de tool calls ===

    this.session.on('tool_approval_requested', (context: any, agent: any, approvalRequest: any) => {
      console.log('[ConnectionManager] üîß Tool approval requested:', approvalRequest);
      // Auto-approve tous les tools backend
      if (this.session && approvalRequest?.approvalItem) {
        console.log('[ConnectionManager] ‚úÖ Auto-approving tool:', approvalRequest.approvalItem);
        this.session.approve(approvalRequest.approvalItem);
      }
    });

    // √âv√©nement quand un tool commence
    this.session.on('agent_tool_start', (context: any, agent: any, tool: any, details: any) => {
      console.log('[ConnectionManager] üîß Tool started:', tool?.name, details);
      this.events.onAssistantTranscript?.('Je v√©rifie...', false);
    });

    // √âv√©nement quand un tool termine
    this.session.on('agent_tool_end', (context: any, agent: any, tool: any, result: any, details: any) => {
      console.log('[ConnectionManager] ‚úÖ Tool completed:', tool?.name, 'result:', result);
    });

    console.log('[ConnectionManager] ‚úÖ Session events configured');
  }

  /**
   * V√©rifie et demande l'acc√®s au microphone
   */
  private async checkMicrophoneAccess(): Promise<MediaStream> {
    console.log('[ConnectionManager] üé§ Checking microphone access...');
    
    try {
      // V√©rifier si l'API est disponible
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error('WebRTC non support√© par ce navigateur');
      }

      // Demander l'acc√®s au microphone
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      console.log('[ConnectionManager] ‚úÖ Microphone access granted');
      console.log('[ConnectionManager] Audio tracks:', stream.getAudioTracks().length);
      
      return stream;
    } catch (error: any) {
      if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
        throw new Error('Acc√®s au microphone refus√©. Veuillez autoriser l\'acc√®s au microphone dans les param√®tres de votre navigateur.');
      } else if (error.name === 'NotFoundError' || error.name === 'DevicesNotFoundError') {
        throw new Error('Aucun microphone d√©tect√©. Veuillez connecter un microphone.');
      } else {
        throw new Error(`Erreur d'acc√®s au microphone: ${error.message}`);
      }
    }
  }

  /**
   * √âtablit la connexion realtime
   */
  async connect(): Promise<void> {
    if (this.status === 'connecting' || this.status === 'connected') {
      console.log('[ConnectionManager] Already connected or connecting');
      return;
    }

    try {
      this.setStatus('connecting');
      console.log('[ConnectionManager] üöÄ Starting connection...');
      
      // Le SDK va g√©rer l'acc√®s au microphone lui-m√™me
      // On ne le fait plus manuellement pour √©viter les conflits

      // 1. R√©cup√©rer la config du backend (mod√®le, voix, instructions)
      // C'est CRITIQUE car le token est cr√©√© avec ce mod√®le
      this.backendConfig = await this.getBackendConfig();
      
      // 2. Obtenir le token ephemeral depuis le backend
      const token = await this.getEphemeralToken();

      // Utiliser la config backend avec fallback sur la config locale
      const effectiveModel = this.backendConfig.model || this.config.model;
      const effectiveVoice = this.backendConfig.voice || this.config.voice;
      const effectiveInstructions = buildVoiceInstructions(
        this.backendConfig.systemInstructions || this.config.systemInstructions,
      );

      // 3. Cr√©er les tools proxy vers le backend
      const backendTools = this.createBackendTools(this.backendConfig.tools || []);
      console.log('[ConnectionManager] Created', backendTools.length, 'backend tools');

      // 4. Cr√©er l'agent avec la config du backend et les tools
      console.log('[ConnectionManager] Creating agent with voice:', effectiveVoice);
      this.agent = new RealtimeAgent({
        name: 'noor-assistant',
        voice: effectiveVoice as any,
        instructions: effectiveInstructions,
        tools: backendTools,
      });

      // 4. Cr√©er la session avec le mod√®le du backend
      // IMPORTANT: Le mod√®le DOIT correspondre √† celui utilis√© pour cr√©er le token
      console.log('[ConnectionManager] Creating session with model:', effectiveModel);
      
      // Cr√©er un √©l√©ment audio pour la sortie et l'attacher au DOM
      const audioElement = this.createAudioElement();
      
      // Cr√©er le transport WebRTC
      const { OpenAIRealtimeWebRTC } = await import('@openai/agents-realtime');
      const transport = new OpenAIRealtimeWebRTC({
        audioElement: audioElement,
      });
      
      // PATCH: Convertir les session.update du SDK v0.3.4 vers le format API
      // Le SDK envoie output_modalities, type: "realtime", audio.input/output
      // L'API attend modalities, voice, turn_detection au niveau racine
      const originalSendEvent = transport.sendEvent.bind(transport);
      transport.sendEvent = (event: any) => {
        if (event?.type === 'session.update' && event?.session) {
          const s = event.session;
          
          // Construire une session convertie
          const converted: Record<string, any> = {};
          
          // IMPORTANT: L'API n'accepte que ['text'] ou ['audio', 'text']
          // Le SDK par d√©faut envoie ['audio'] qui est INVALIDE
          // On force TOUJOURS ['audio', 'text']
          converted.modalities = ['audio', 'text'];
          
          // instructions
          if (s.instructions) {
            converted.instructions = s.instructions;
          }
          
          // voice (peut √™tre dans audio.output.voice ou directement)
          if (s.audio?.output?.voice) {
            converted.voice = s.audio.output.voice;
          } else if (s.voice) {
            converted.voice = s.voice;
          }
          
          // turn_detection (peut √™tre dans audio.input.turnDetection)
          if (s.audio?.input?.turnDetection) {
            const td = s.audio.input.turnDetection;
            converted.turn_detection = {
              type: td.type || 'server_vad',
              threshold: td.threshold,
              prefix_padding_ms: td.prefixPaddingMs || td.prefix_padding_ms,
              silence_duration_ms: td.silenceDurationMs || td.silence_duration_ms,
              create_response: td.createResponse ?? td.create_response ?? true,
            };
          }
          
          // tools
          if (s.tools && s.tools.length > 0) {
            converted.tools = s.tools;
          }
          
          // tool_choice
          if (s.toolChoice || s.tool_choice) {
            converted.tool_choice = s.toolChoice || s.tool_choice;
          }
          
          console.log('[ConnectionManager] üì§ Session.update converti:', {
            from: Object.keys(s).filter(k => k !== 'type' && k !== 'tracing'),
            to: Object.keys(converted),
            hasTools: !!converted.tools?.length,
          });
          
          // Si vide apr√®s conversion, ne pas envoyer
          if (Object.keys(converted).length === 0) {
            console.log('[ConnectionManager] ‚ö†Ô∏è Session.update ignor√© (vide)');
            return;
          }
          
          return originalSendEvent({
            type: 'session.update',
            session: converted,
          });
        }
        return originalSendEvent(event);
      };
      
      // Configuration de la session
      this.session = new RealtimeSession(this.agent, {
        transport: transport,
        model: effectiveModel,
        tracingDisabled: true,
      });

      // 5. Configurer les √©v√©nements AVANT de se connecter
      this.setupSessionEvents();

      // 6. Se connecter - Le SDK va automatiquement:
      //    - Demander l'acc√®s au microphone
      //    - Cr√©er un √©l√©ment <audio> pour la sortie
      //    - √âtablir la connexion WebRTC avec OpenAI
      console.log('[ConnectionManager] üåê Connecting to OpenAI Realtime API...');
      
      // Intercepteur temporaire pour diagnostiquer les erreurs
      const originalFetch = globalThis.fetch;
      globalThis.fetch = async (input, init) => {
        const url = typeof input === 'string' ? input : input instanceof URL ? input.toString() : (input as Request).url;
        if (url.includes('openai.com/v1/realtime')) {
          let bodyPreview = '[non-string]';
          let bodyLength = 0;
          if (typeof init?.body === 'string') {
            bodyPreview = init.body.substring(0, 80);
            bodyLength = init.body.length;
          }

          // Forcer l'en-t√™te Beta requis par l'API Realtime
          const headers = new Headers(init?.headers || {});
          if (!headers.has('OpenAI-Beta')) {
            headers.set('OpenAI-Beta', 'realtime=v1');
          }
          init = {
            ...init,
            headers,
          };

          console.log('[ConnectionManager] üì° OpenAI Realtime request:', {
            url,
            method: init?.method,
            contentType: headers.get('Content-Type'),
            bodyLength,
            bodyPreview,
          });
          try {
            const response = await originalFetch(input, init);
            console.log('[ConnectionManager] üì° OpenAI Response status:', response.status);
            if (!response.ok) {
              // Clone pour pouvoir lire le body sans le consommer
              const cloned = response.clone();
              const errorBody = await cloned.text();
              console.error('[ConnectionManager] ‚ùå OpenAI Error body:', errorBody);
            }
            return response;
          } catch (e) {
            console.error('[ConnectionManager] ‚ùå Fetch error:', e);
            throw e;
          }
        }
        return originalFetch(input, init);
      };
      
      const connectOptions: RealtimeSessionConnectOptions = {
        apiKey: token,
      };

      try {
        await this.session.connect(connectOptions);
      } finally {
        // Restaurer fetch original
        globalThis.fetch = originalFetch;
      }

      console.log('[ConnectionManager] ‚úÖ Connected successfully!');
      console.log('[ConnectionManager] üéôÔ∏è Microphone and audio output should be active');
      
      // NOTE: La session est d√©j√† configur√©e par le backend lors de la cr√©ation du token
      // On n'envoie PAS de session.update suppl√©mentaire pour √©viter les conflits
      // Les modalities, voice, instructions sont d√©j√† configur√©s
      console.log('[ConnectionManager] ‚úÖ Session configur√©e par le backend, pas de session.update suppl√©mentaire');
      
      this.setStatus('connected');
      this.reconnectAttempts = 0;

    } catch (error) {
      console.error('[ConnectionManager] ‚ùå Connection failed:', error);
      this.setStatus('error');
      this.events.onError?.(error as Error);

      if (this.reconnectAttempts < MAX_RECONNECT_ATTEMPTS) {
        this.attemptReconnect();
      }
    }
  }

  private async attemptReconnect(): Promise<void> {
    this.reconnectAttempts++;
    console.log(`[ConnectionManager] üîÑ Reconnecting (${this.reconnectAttempts}/${MAX_RECONNECT_ATTEMPTS})`);
    
    this.setStatus('reconnecting');
    await new Promise(resolve => setTimeout(resolve, RECONNECT_DELAY));
    
    try {
      await this.connect();
    } catch (error) {
      console.error('[ConnectionManager] Reconnect failed:', error);
      if (this.reconnectAttempts >= MAX_RECONNECT_ATTEMPTS) {
        this.setStatus('error');
        this.events.onError?.(new Error('Impossible de se reconnecter'));
      }
    }
  }

  async disconnect(): Promise<void> {
    console.log('[ConnectionManager] üîå Disconnecting...');

    if (this.session) {
      try {
        this.session.close();
      } catch (error) {
        console.error('[ConnectionManager] Error closing session:', error);
      }
      this.session = null;
    }

    // Nettoyer l'√©l√©ment audio
    if (this.audioElement) {
      this.audioElement.remove();
      console.log('[ConnectionManager] üîá Audio element removed');
      this.audioElement = null;
    } else {
      const fallbackAudio = document.getElementById('realtime-audio-output');
      if (fallbackAudio) {
        fallbackAudio.remove();
        console.log('[ConnectionManager] üîá Audio element removed (fallback)');
      }
    }

    this.agent = null;
    this.currentTranscript = '';
    this.setStatus('disconnected');
    this.setAudioState('idle');
    console.log('[ConnectionManager] ‚úÖ Disconnected');
  }

  async sendTextMessage(text: string): Promise<void> {
    if (!this.session || this.status !== 'connected') {
      throw new Error('Session non connect√©e');
    }

    try {
      console.log('[ConnectionManager] üí¨ Sending text message:', text);
      this.session.sendMessage(text);
    } catch (error) {
      console.error('[ConnectionManager] Failed to send message:', error);
      throw error;
    }
  }

  async interrupt(): Promise<void> {
    if (!this.session || this.status !== 'connected') return;

    try {
      console.log('[ConnectionManager] ‚èπÔ∏è Interrupting...');
      this.session.interrupt();
      this.setAudioState('idle');
    } catch (error) {
      console.error('[ConnectionManager] Failed to interrupt:', error);
    }
  }

  mute(muted: boolean): void {
    if (!this.session) return;
    
    try {
      this.session.mute(muted);
      console.log('[ConnectionManager] üîá Mute:', muted);
    } catch (error) {
      console.error('[ConnectionManager] Failed to mute:', error);
    }
  }

  getStatus(): ConnectionStatus {
    return this.status;
  }

  getAudioState(): AudioState {
    return this.audioState;
  }

  isConnected(): boolean {
    return this.status === 'connected';
  }

  isMuted(): boolean | null {
    return this.session?.muted ?? null;
  }

  updateConfig(config: Partial<RealtimeConfig>): void {
    this.config = { ...this.config, ...config };
  }
}

export function createConnectionManager(options: ConnectionManagerOptions): ConnectionManager {
  return new ConnectionManager(options);
}
